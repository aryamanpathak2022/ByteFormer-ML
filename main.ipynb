{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "535enlBoMgO3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer,Dense,Flatten,Dropout,LayerNormalization\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input,Embedding,Conv1D,Flatten,Dense,Dropout,LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### `PatchEmbedding` \n",
        "- divides input images into non-overlapping patches,flattens them,and projects each patch into a specified embedding dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j0Fa289OQluL"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(Layer):\n",
        "    def __init__(self,patch_size,embed_dim):\n",
        "        super(PatchEmbedding,self).__init__()\n",
        "        self.patch_size=patch_size\n",
        "        self.embed_dim=embed_dim\n",
        "        self.projection=Dense(embed_dim)\n",
        "\n",
        "    def call(self,images):\n",
        "        # images -> (batch_size,height,width,channels)\n",
        "        batch_size=tf.shape(images)[0] \n",
        "        patches=tf.image.extract_patches(images=images,sizes=[1,self.patch_size,self.patch_size,1],strides=[1,self.patch_size,self.patch_size,1],rates=[1,1,1,1],padding='VALID') # (patch_size,patch_size)\n",
        "        patch_dims=patches.shape[-1]\n",
        "        patches=tf.reshape(patches,[batch_size,-1,patch_dims]) # (batch_size,num_patches,patch_dims)\n",
        "        embeddings=self.projection(patches) # (batch_size,num_patches,embed_dim)\n",
        "        return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `PositionalEncoding`\n",
        "- adds positional information to the input embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UqtyAgtmQpIQ"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(Layer):\n",
        "    def __init__(self,num_patches,embed_dim):\n",
        "        super(PositionalEncoding,self).__init__()\n",
        "        self.pos_encoding=self.positional_encoding(num_patches,embed_dim)\n",
        "\n",
        "    def positional_encoding(self,num_patches,embed_dim):\n",
        "        positions=tf.range(num_patches,dtype=tf.float32)[:,tf.newaxis]  # (num_patches,1)\n",
        "        div_term=tf.exp(tf.range(0,embed_dim,2,dtype=tf.float32)*-(tf.math.log(10000.0)/embed_dim))\n",
        "        even_indices=tf.sin(positions*div_term)\n",
        "        odd_indices=tf.cos(positions*div_term)\n",
        "        pos_encoding=tf.concat([even_indices,odd_indices],axis=1)\n",
        "        return pos_encoding[:,:embed_dim]  # (num_patches,embed_dim)\n",
        "\n",
        "    def call(self,x): #incorporate positional information\n",
        "        return x+self.pos_encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### `TransformerEncoderBlock`\n",
        "- combines multi-head self-attention,feed-forward neural networks,residual connections,and normalization layers to process input sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2XenNMbgQqq6"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(Layer):\n",
        "    def __init__(self,embed_dim,num_heads,ff_dim,dropout_rate=0.1):\n",
        "        super(TransformerEncoderBlock,self).__init__()\n",
        "        self.att=tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
        "        self.ffn=tf.keras.Sequential([Dense(ff_dim,activation='relu'),# (batch_size,seq_len,ff_dim)\n",
        "                                        Dense(embed_dim)])  # (batch_size,seq_len,embed_dim)\n",
        "        self.layernorm1=LayerNormalization(epsilon=1e-6) # (batch_size,seq_len,embed_dim)\n",
        "        self.layernorm2=LayerNormalization(epsilon=1e-6) # (batch_size,seq_len,embed_dim)\n",
        "        self.dropout1=Dropout(dropout_rate) \n",
        "        self.dropout2=Dropout(dropout_rate) \n",
        "\n",
        "    def call(self,inputs,training=None):\n",
        "        attn_output=self.att(inputs,inputs)  #  (batch_size,seq_len,embed_di)\n",
        "        attn_output=self.dropout1(attn_output,training=training) \n",
        "        out1=self.layernorm1(inputs+attn_output)  # (batch_size,seq_len,embed_dim)\n",
        "\n",
        "        ffn_output=self.ffn(out1)  # (batch_size,seq_len,embed_di)\n",
        "        ffn_output=self.dropout2(ffn_output,training=training) \n",
        "\n",
        "        return self.layernorm2(out1+ffn_output)  # (batch_size,seq_len,embed_di)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0DZ-m49yQsO_"
      },
      "outputs": [],
      "source": [
        "def create_vit_model(input_shape,patch_size,embed_dim,num_heads,ff_dim,num_layers,num_classes):\n",
        "    inputs=tf.keras.Input(shape=input_shape)\n",
        "    patches=PatchEmbedding(patch_size,embed_dim)(inputs)\n",
        "    num_patches=(input_shape[0] // patch_size)*(input_shape[1] // patch_size)\n",
        "    positions=PositionalEncoding(num_patches,embed_dim)(patches)\n",
        "    x=positions\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        x=TransformerEncoderBlock(embed_dim,num_heads,ff_dim)(x)\n",
        "\n",
        "    x=LayerNormalization(epsilon=1e-6)(x)\n",
        "    x=Flatten()(x)\n",
        "    x=Dense(ff_dim,activation='relu')(x)\n",
        "    x=Dropout(0.1)(x)\n",
        "    outputs=Dense(num_classes,activation='softmax')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs,outputs=outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the $\\texttt{cifar10}$ dataset and normalising the values to $(0,1)$<br>\n",
        "The y values are one hot encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jNEcKc_RCTB",
        "outputId": "d4fadff2-169b-485e-8373-5419311fb004"
      },
      "outputs": [],
      "source": [
        "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
        "x_train=x_train.astype(\"int32\")/255.0\n",
        "x_test=x_test.astype(\"int32\")/255.0\n",
        "\n",
        "y_train=to_categorical(y_train,10)\n",
        "y_test=to_categorical(y_test,10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the VIT model with the following Model parameters\n",
        "- Input Shape: `(32, 32, 3)`\n",
        "- Patch Size: `4`\n",
        "- Embedding Dimension: `64`\n",
        "- Number of Attention Heads: `4`\n",
        "- Feed-Forward Network Dimension: `128`\n",
        "- Number of Transformer Encoder Layers: `8`\n",
        "- Number of Classes: `10`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u2Jo62nFS8xq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From f:\\not_delete\\conda\\envs\\ap\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vit_model=create_vit_model(\n",
        "    input_shape=(32,32,3),\n",
        "    patch_size=4,\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=128,\n",
        "    num_layers=8,\n",
        "    num_classes=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "FVVFOV2ZRHvS",
        "outputId": "36e8dede-eeea-444d-b73d-8da2bade7ff0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_embedding                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEmbedding</span>)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ positional_encoding             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_embedding                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m3,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mPatchEmbedding\u001b[0m)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ positional_encoding             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m83,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m83,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m83,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m83,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m83,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m83,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m83,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m83,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m524,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,194,570</span> (4.56 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,194,570\u001b[0m (4.56 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,194,570</span> (4.56 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,194,570\u001b[0m (4.56 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "vit_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compiling the model with loss function as $\\texttt{crossentropy}$ and $\\texttt{Adam}$ optimizer with learning rate $3*10^{-4}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AjInoEsS9eK",
        "outputId": "12cfc911-131e-482f-b258-190d7281b2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 43ms/step - accuracy: 0.1885 - loss: 2.1956 - val_accuracy: 0.3933 - val_loss: 1.7012\n",
            "Epoch 2/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.3994 - loss: 1.6703 - val_accuracy: 0.4722 - val_loss: 1.4432\n",
            "Epoch 3/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.4677 - loss: 1.4815 - val_accuracy: 0.5050 - val_loss: 1.3623\n",
            "Epoch 4/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.5169 - loss: 1.3435 - val_accuracy: 0.5270 - val_loss: 1.3179\n",
            "Epoch 5/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.5535 - loss: 1.2453 - val_accuracy: 0.5601 - val_loss: 1.2388\n",
            "Epoch 6/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.5884 - loss: 1.1437 - val_accuracy: 0.5701 - val_loss: 1.2105\n",
            "Epoch 7/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.6187 - loss: 1.0616 - val_accuracy: 0.5866 - val_loss: 1.1761\n",
            "Epoch 8/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.6515 - loss: 0.9823 - val_accuracy: 0.5897 - val_loss: 1.1754\n",
            "Epoch 9/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.6820 - loss: 0.8923 - val_accuracy: 0.6139 - val_loss: 1.1296\n",
            "Epoch 10/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.7079 - loss: 0.8162 - val_accuracy: 0.6072 - val_loss: 1.1479\n",
            "Epoch 11/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.7353 - loss: 0.7459 - val_accuracy: 0.6174 - val_loss: 1.1534\n",
            "Epoch 12/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.7587 - loss: 0.6771 - val_accuracy: 0.6143 - val_loss: 1.1518\n",
            "Epoch 13/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.7878 - loss: 0.6050 - val_accuracy: 0.6208 - val_loss: 1.1984\n",
            "Epoch 14/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.8025 - loss: 0.5509 - val_accuracy: 0.6164 - val_loss: 1.2047\n",
            "Epoch 15/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.8305 - loss: 0.4912 - val_accuracy: 0.6277 - val_loss: 1.2375\n",
            "Epoch 16/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.8441 - loss: 0.4430 - val_accuracy: 0.6243 - val_loss: 1.2498\n",
            "Epoch 17/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.8594 - loss: 0.4011 - val_accuracy: 0.6139 - val_loss: 1.3162\n",
            "Epoch 18/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.8763 - loss: 0.3620 - val_accuracy: 0.6215 - val_loss: 1.3407\n",
            "Epoch 19/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.8864 - loss: 0.3267 - val_accuracy: 0.6194 - val_loss: 1.3917\n",
            "Epoch 20/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.8921 - loss: 0.3110 - val_accuracy: 0.6194 - val_loss: 1.4311\n",
            "Epoch 21/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9063 - loss: 0.2765 - val_accuracy: 0.6214 - val_loss: 1.4368\n",
            "Epoch 22/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9096 - loss: 0.2599 - val_accuracy: 0.6184 - val_loss: 1.4640\n",
            "Epoch 23/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9202 - loss: 0.2303 - val_accuracy: 0.6230 - val_loss: 1.6293\n",
            "Epoch 24/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9252 - loss: 0.2180 - val_accuracy: 0.6185 - val_loss: 1.6398\n",
            "Epoch 25/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9305 - loss: 0.2050 - val_accuracy: 0.6177 - val_loss: 1.6740\n",
            "Epoch 26/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9329 - loss: 0.1887 - val_accuracy: 0.6248 - val_loss: 1.6798\n",
            "Epoch 27/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9375 - loss: 0.1778 - val_accuracy: 0.6181 - val_loss: 1.7021\n",
            "Epoch 28/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9405 - loss: 0.1738 - val_accuracy: 0.6190 - val_loss: 1.7979\n",
            "Epoch 29/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9449 - loss: 0.1615 - val_accuracy: 0.6198 - val_loss: 1.7973\n",
            "Epoch 30/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9459 - loss: 0.1566 - val_accuracy: 0.6166 - val_loss: 1.8203\n",
            "Epoch 31/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9459 - loss: 0.1534 - val_accuracy: 0.6240 - val_loss: 1.8033\n",
            "Epoch 32/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9501 - loss: 0.1462 - val_accuracy: 0.6171 - val_loss: 1.8862\n",
            "Epoch 33/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9497 - loss: 0.1471 - val_accuracy: 0.6235 - val_loss: 1.9067\n",
            "Epoch 34/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9554 - loss: 0.1286 - val_accuracy: 0.6183 - val_loss: 1.9763\n",
            "Epoch 35/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9517 - loss: 0.1394 - val_accuracy: 0.6225 - val_loss: 1.9338\n",
            "Epoch 36/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9554 - loss: 0.1298 - val_accuracy: 0.6197 - val_loss: 1.8643\n",
            "Epoch 37/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9564 - loss: 0.1279 - val_accuracy: 0.6204 - val_loss: 2.0141\n",
            "Epoch 38/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9601 - loss: 0.1192 - val_accuracy: 0.6135 - val_loss: 1.8771\n",
            "Epoch 39/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9565 - loss: 0.1219 - val_accuracy: 0.6176 - val_loss: 1.8722\n",
            "Epoch 40/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9615 - loss: 0.1148 - val_accuracy: 0.6190 - val_loss: 2.0326\n",
            "Epoch 41/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9601 - loss: 0.1169 - val_accuracy: 0.6161 - val_loss: 2.0153\n",
            "Epoch 42/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9630 - loss: 0.1098 - val_accuracy: 0.6188 - val_loss: 2.1609\n",
            "Epoch 43/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9605 - loss: 0.1132 - val_accuracy: 0.6166 - val_loss: 2.0034\n",
            "Epoch 44/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9647 - loss: 0.1042 - val_accuracy: 0.6209 - val_loss: 2.0775\n",
            "Epoch 45/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9604 - loss: 0.1119 - val_accuracy: 0.6151 - val_loss: 2.1397\n",
            "Epoch 46/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9655 - loss: 0.0989 - val_accuracy: 0.6163 - val_loss: 2.1386\n",
            "Epoch 47/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9670 - loss: 0.0967 - val_accuracy: 0.6221 - val_loss: 2.1048\n",
            "Epoch 48/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9659 - loss: 0.0965 - val_accuracy: 0.6211 - val_loss: 2.1713\n",
            "Epoch 49/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9669 - loss: 0.0955 - val_accuracy: 0.6261 - val_loss: 2.1615\n",
            "Epoch 50/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9665 - loss: 0.0933 - val_accuracy: 0.6117 - val_loss: 2.0558\n",
            "Epoch 51/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9674 - loss: 0.0940 - val_accuracy: 0.6209 - val_loss: 2.3193\n",
            "Epoch 52/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9667 - loss: 0.0956 - val_accuracy: 0.6191 - val_loss: 2.3443\n",
            "Epoch 53/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9686 - loss: 0.0899 - val_accuracy: 0.6200 - val_loss: 2.2790\n",
            "Epoch 54/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9705 - loss: 0.0884 - val_accuracy: 0.6150 - val_loss: 2.2151\n",
            "Epoch 55/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9706 - loss: 0.0855 - val_accuracy: 0.6224 - val_loss: 2.3045\n",
            "Epoch 56/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9697 - loss: 0.0858 - val_accuracy: 0.6198 - val_loss: 2.2600\n",
            "Epoch 57/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9727 - loss: 0.0795 - val_accuracy: 0.6226 - val_loss: 2.3095\n",
            "Epoch 58/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9739 - loss: 0.0760 - val_accuracy: 0.6205 - val_loss: 2.2675\n",
            "Epoch 59/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9698 - loss: 0.0885 - val_accuracy: 0.6222 - val_loss: 2.3095\n",
            "Epoch 60/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9709 - loss: 0.0867 - val_accuracy: 0.6149 - val_loss: 2.2233\n",
            "Epoch 61/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9725 - loss: 0.0790 - val_accuracy: 0.6086 - val_loss: 2.3266\n",
            "Epoch 62/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9709 - loss: 0.0855 - val_accuracy: 0.6173 - val_loss: 2.3653\n",
            "Epoch 63/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9723 - loss: 0.0791 - val_accuracy: 0.6175 - val_loss: 2.5404\n",
            "Epoch 64/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9711 - loss: 0.0836 - val_accuracy: 0.6228 - val_loss: 2.3573\n",
            "Epoch 65/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9746 - loss: 0.0736 - val_accuracy: 0.6179 - val_loss: 2.3240\n",
            "Epoch 66/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9758 - loss: 0.0701 - val_accuracy: 0.6175 - val_loss: 2.3251\n",
            "Epoch 67/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.9733 - loss: 0.0745 - val_accuracy: 0.6234 - val_loss: 2.4704\n",
            "Epoch 68/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.9754 - loss: 0.0706 - val_accuracy: 0.6227 - val_loss: 2.3775\n",
            "Epoch 69/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9742 - loss: 0.0745 - val_accuracy: 0.6154 - val_loss: 2.2213\n",
            "Epoch 70/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9765 - loss: 0.0697 - val_accuracy: 0.6110 - val_loss: 2.4046\n",
            "Epoch 71/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9738 - loss: 0.0795 - val_accuracy: 0.6259 - val_loss: 2.6510\n",
            "Epoch 72/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9743 - loss: 0.0754 - val_accuracy: 0.6242 - val_loss: 2.4781\n",
            "Epoch 73/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9771 - loss: 0.0639 - val_accuracy: 0.6207 - val_loss: 2.4035\n",
            "Epoch 74/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9793 - loss: 0.0636 - val_accuracy: 0.6198 - val_loss: 2.4717\n",
            "Epoch 75/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9752 - loss: 0.0724 - val_accuracy: 0.6200 - val_loss: 2.4980\n",
            "Epoch 76/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9783 - loss: 0.0653 - val_accuracy: 0.6146 - val_loss: 2.3196\n",
            "Epoch 77/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9769 - loss: 0.0659 - val_accuracy: 0.6228 - val_loss: 2.3756\n",
            "Epoch 78/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9758 - loss: 0.0692 - val_accuracy: 0.6253 - val_loss: 2.4369\n",
            "Epoch 79/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9778 - loss: 0.0630 - val_accuracy: 0.6201 - val_loss: 2.6153\n",
            "Epoch 80/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9797 - loss: 0.0605 - val_accuracy: 0.6075 - val_loss: 2.5372\n",
            "Epoch 81/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9767 - loss: 0.0672 - val_accuracy: 0.6172 - val_loss: 2.4830\n",
            "Epoch 82/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9813 - loss: 0.0586 - val_accuracy: 0.6220 - val_loss: 2.4912\n",
            "Epoch 83/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9779 - loss: 0.0667 - val_accuracy: 0.6197 - val_loss: 2.4186\n",
            "Epoch 84/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9797 - loss: 0.0628 - val_accuracy: 0.6209 - val_loss: 2.4442\n",
            "Epoch 85/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9782 - loss: 0.0634 - val_accuracy: 0.6151 - val_loss: 2.4876\n",
            "Epoch 86/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9809 - loss: 0.0577 - val_accuracy: 0.6174 - val_loss: 2.6162\n",
            "Epoch 87/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9804 - loss: 0.0588 - val_accuracy: 0.6230 - val_loss: 2.6276\n",
            "Epoch 88/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9822 - loss: 0.0548 - val_accuracy: 0.6182 - val_loss: 2.5671\n",
            "Epoch 89/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9804 - loss: 0.0586 - val_accuracy: 0.6219 - val_loss: 2.6363\n",
            "Epoch 90/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9811 - loss: 0.0552 - val_accuracy: 0.6244 - val_loss: 2.5895\n",
            "Epoch 91/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9792 - loss: 0.0600 - val_accuracy: 0.6227 - val_loss: 2.4880\n",
            "Epoch 92/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9807 - loss: 0.0582 - val_accuracy: 0.6167 - val_loss: 2.6701\n",
            "Epoch 93/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9813 - loss: 0.0576 - val_accuracy: 0.6170 - val_loss: 2.4986\n",
            "Epoch 94/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9810 - loss: 0.0569 - val_accuracy: 0.6198 - val_loss: 2.5934\n",
            "Epoch 95/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9805 - loss: 0.0596 - val_accuracy: 0.6234 - val_loss: 2.6905\n",
            "Epoch 96/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9810 - loss: 0.0567 - val_accuracy: 0.6186 - val_loss: 2.6339\n",
            "Epoch 97/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9790 - loss: 0.0622 - val_accuracy: 0.6177 - val_loss: 2.7675\n",
            "Epoch 98/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9827 - loss: 0.0498 - val_accuracy: 0.6192 - val_loss: 2.7088\n",
            "Epoch 99/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9814 - loss: 0.0524 - val_accuracy: 0.6196 - val_loss: 2.6332\n",
            "Epoch 100/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9823 - loss: 0.0511 - val_accuracy: 0.6294 - val_loss: 2.7839\n"
          ]
        }
      ],
      "source": [
        "vit_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "history=vit_model.fit(x_train,y_train,batch_size=64,epochs=100,validation_split=0.2,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are implementing the $\\texttt{ByteFormer}$ on cifar10 dataset.<br>\n",
        "For implementing this, we are converting the RGB images $(32*32*3)$ to $(3072,1)$ with each element representing the corresponding byte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vit_model.save(\"vit.keras\")  ## saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6kVUI16qZgU",
        "outputId": "6171fdd6-d12e-48f8-e23a-a5818b899536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 3072)\n",
            "(10000, 3072)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
        "# flatten 32x32x3 into a 1D array of 3072 bytes\n",
        "x_train_bytes=x_train.reshape(x_train.shape[0],-1).astype(np.uint8)  #(3072,1)\n",
        "x_test_bytes=x_test.reshape(x_test.shape[0],-1).astype(np.uint8)\n",
        "\n",
        "y_train=to_categorical(y_train,10)\n",
        "y_test=to_categorical(y_test,10)\n",
        "\n",
        "first_image_bytes=x_train_bytes[0]\n",
        "x_train_bytes=x_train_bytes.astype(np.float32)\n",
        "y_train=y_train.astype(np.float32)\n",
        "\n",
        "print(x_train_bytes.shape) # (50000,3072)\n",
        "print(x_test_bytes.shape) # (10000,3072)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### `PositionalEncoding` \n",
        "- introduces positional information into the input embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZoUmTeIvBz_"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(Layer):\n",
        "    def __init__(self,num_patches,embed_dim):\n",
        "        super(PositionalEncoding,self).__init__()\n",
        "        self.num_patches=num_patches\n",
        "        self.embed_dim=embed_dim\n",
        "        self.pos_encoding=self.positional_encoding(num_patches,embed_dim)\n",
        "\n",
        "    def positional_encoding(self,num_patches,embed_dim):\n",
        "        # indexes (0 to num_patches-1)\n",
        "        positions=tf.range(num_patches,dtype=tf.float32)[:,tf.newaxis]  # (num_patches,1)\n",
        "        div_term=tf.exp(tf.range(0,embed_dim,2,dtype=tf.float32)*-(tf.math.log(10000.0)/embed_dim))  # (embed_dim // 2)\n",
        "        # positional encoding using sin and cos\n",
        "        pos_encoding=tf.concat([tf.sin(positions*div_term),tf.cos(positions*div_term)],axis=1)  # (num_patches,embed_dim)\n",
        "        pos_encoding=tf.expand_dims(pos_encoding,axis=0)  # (1,num_patches,embed_dim)\n",
        "        return pos_encoding\n",
        "\n",
        "    def call(self,x):\n",
        "        # x - (batch_size,num_patches,embed_dim) -> (?,3072,64)\n",
        "        # applying positional encoding on the last dimension (64)\n",
        "        pos_encoding_resized=tf.tile(self.pos_encoding,multiples=[tf.shape(x)[0],1,1])  # (batch_size,num_patches,embed_dim)\n",
        "        return x+pos_encoding_resized \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### `TransformerEncoderBlock`\n",
        "As mentioned in the paper, Transfomer blocks of both ViT and ByteFormers is the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0_WLu5wtuJ9R"
      },
      "outputs": [],
      "source": [
        "def create_vit_byte_model(input_shape,byte_vocab_size,byte_embed_dim,conv_filters,embed_dim,num_heads,ff_dim,num_layers,num_classes):\n",
        "    inputs=Input(shape=input_shape)\n",
        "    x=Embedding(input_dim=byte_vocab_size,output_dim=byte_embed_dim)(inputs)  # (3072,32)\n",
        "    x=Conv1D(filters=conv_filters,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\")(x)  # (3072,32)\n",
        "    x=PositionalEncoding(num_patches=1536,embed_dim=embed_dim)(x)  \n",
        "    for _ in range(num_layers):\n",
        "        x=TransformerEncoderBlock(embed_dim,num_heads,ff_dim)(x)  \n",
        "\n",
        "    x=LayerNormalization(epsilon=1e-6)(x)  \n",
        "    x=Flatten()(x)  \n",
        "    x=Dense(ff_dim,activation=\"relu\")(x)  \n",
        "    x=Dropout(0.005)(x)  \n",
        "    outputs=Dense(num_classes,activation=\"softmax\")(x)\n",
        "    return Model(inputs=inputs,outputs=outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Creating the ByteFomer with the Following Parameters:\n",
        "\n",
        "- Input Shape: `(3072,)`  \n",
        "- Byte Vocabulary Size: `256`  \n",
        "- Byte Embedding Dimension: `128`  \n",
        "- Number of Convolutional Filters: `32`  \n",
        "- Embedding Dimension: `32`  \n",
        "- Number of Attention Heads: `4`  \n",
        "- Feed-Forward Network Dimension: `256`  \n",
        "- Number of Transformer Encoder Layers: `4`  \n",
        "- Number of Classes: `10`  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "ENyrajaVuLaL",
        "outputId": "21442950-bf85-42ee-d699-e9212af31dcb"
      },
      "outputs": [],
      "source": [
        "model=create_vit_byte_model(\n",
        "    input_shape=(3072,),\n",
        "    byte_vocab_size=256,\n",
        "    byte_embed_dim=128,\n",
        "    conv_filters=32,\n",
        "    embed_dim=32,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    num_layers=4,\n",
        "    num_classes=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compiling the model with Adam optimizer and crossentropy as loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ positional_encoding_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,600</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,600</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,600</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_15    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,600</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderBlock</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_34          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49152</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,583,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_14 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m32,768\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │        \u001b[38;5;34m12,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ positional_encoding_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │        \u001b[38;5;34m33,600\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │        \u001b[38;5;34m33,600\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │        \u001b[38;5;34m33,600\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_block_15    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │        \u001b[38;5;34m33,600\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoderBlock\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_34          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m64\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49152\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m12,583,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_50 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,765,290</span> (48.70 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,765,290\u001b[0m (48.70 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,765,290</span> (48.70 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,765,290\u001b[0m (48.70 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fitting the model with batch_size=$16$ for $100$ epochs with train,val split as $20\\%$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqJVtghQv5Zi",
        "outputId": "2c543c5c-8a1e-43b9-c803-4a2b514182d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 244ms/step - accuracy: 0.0982 - loss: 4.0015 - val_accuracy: 0.0997 - val_loss: 2.3027\n",
            "Epoch 2/100\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m667s\u001b[0m 245ms/step - accuracy: 0.1032 - loss: 2.3028 - val_accuracy: 0.0952 - val_loss: 2.3028\n",
            "Epoch 3/100\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 234ms/step - accuracy: 0.0992 - loss: 2.3028 - val_accuracy: 0.1003 - val_loss: 2.3027\n",
            "Epoch 4/100\n",
            "\u001b[1m1080/2500\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5:01\u001b[0m 212ms/step - accuracy: 0.0963 - loss: 2.3027"
          ]
        }
      ],
      "source": [
        "history=model.fit(x_train_bytes,y_train,batch_size=16, epochs=100, validation_split=0.2,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTNa6Wsn7aRA"
      },
      "outputs": [],
      "source": [
        "# model.save(\"Byteformer.keras\")  ## saving the model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ap",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
